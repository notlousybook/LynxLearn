{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "name": "LousyBookML Benchmark",
            "provenance": [],
            "collapsed_sections": []
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LousyBookML Unified Benchmark\n",
                "\n",
                "This notebook benchmarks **PyTorch + TensorFlow + LousyBookML** using the **EXACT same dataset** for fair comparison.\n",
                "\n",
                "## Instructions:\n",
                "1. Run all cells (Runtime → Run all)\n",
                "2. Download `colab_benchmark.json` when complete (contains BOTH data and results)\n",
                "3. Place in your local `benchmark/` folder\n",
                "4. Run `python benchmark/benchmark_runner.py` locally"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install PyTorch and TensorFlow\n",
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q\n",
                "!pip install tensorflow -q\n",
                "print(\"[OK] PyTorch and TensorFlow installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Upload LousyBookML Package\n",
                "\n",
                "Upload the `lousybookml` folder from your local machine:\n",
                "1. Click the folder icon on the left sidebar\n",
                "2. Click the upload button\n",
                "3. Select your local `lousybookml` folder\n",
                "\n",
                "Or run the cell below to install from the current directory if you've already uploaded it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Add current directory to path for lousybookml\n",
                "import sys\n",
                "sys.path.insert(0, '/content')\n",
                "\n",
                "# Try to import lousybookml\n",
                "try:\n",
                "    import lousybookml\n",
                "    print(\"[OK] LousyBookML loaded\")\n",
                "except ImportError:\n",
                "    print(\"[!] LousyBookML not found. Please upload the lousybookml folder.\")\n",
                "    print(\"    See instructions in the cell above.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Run Unified Benchmark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import time\n",
                "import json\n",
                "\n",
                "# ==============================================================================\n",
                "# DATA GENERATION (Fixed seed for reproducibility)\n",
                "# ==============================================================================\n",
                "\n",
                "def generate_and_save_data(n_samples=5000, n_features=20, noise=0.5, random_state=42):\n",
                "    \"\"\"Generate synthetic data with fixed seed and save to JSON.\"\"\"\n",
                "    np.random.seed(random_state)\n",
                "    \n",
                "    X = np.random.randn(n_samples, n_features)\n",
                "    true_weights = np.random.randn(n_features) * 2\n",
                "    true_bias = 5.0\n",
                "    y = X @ true_weights + true_bias + np.random.randn(n_samples) * noise\n",
                "    \n",
                "    indices = np.random.permutation(n_samples)\n",
                "    split_idx = int(0.8 * n_samples)\n",
                "    train_idx, test_idx = indices[:split_idx], indices[split_idx:]\n",
                "    \n",
                "    X_train, X_test = X[train_idx].tolist(), X[test_idx].tolist()\n",
                "    y_train, y_test = y[train_idx].tolist(), y[test_idx].tolist()\n",
                "    \n",
                "    data = {\n",
                "        'n_samples': n_samples,\n",
                "        'n_features': n_features,\n",
                "        'noise': noise,\n",
                "        'random_state': random_state,\n",
                "        'true_weights': true_weights.tolist(),\n",
                "        'true_bias': float(true_bias),\n",
                "        'X_train': X_train,\n",
                "        'X_test': X_test,\n",
                "        'y_train': y_train,\n",
                "        'y_test': y_test,\n",
                "        'train_size': len(y_train),\n",
                "        'test_size': len(y_test)\n",
                "    }\n",
                "    \n",
                "    print(f\"[OK] Generated benchmark data\")\n",
                "    print(f\"     Samples: {n_samples}, Features: {n_features}\")\n",
                "    print(f\"     Train: {len(y_train)}, Test: {len(y_test)}\")\n",
                "    \n",
                "    return data\n",
                "\n",
                "# Generate data\n",
                "print(\"Generating dataset with fixed seed...\")\n",
                "data_info = generate_and_save_data()\n",
                "\n",
                "# Load data as numpy arrays\n",
                "X_train = np.array(data_info['X_train'])\n",
                "X_test = np.array(data_info['X_test'])\n",
                "y_train = np.array(data_info['y_train'])\n",
                "y_test = np.array(data_info['y_test'])\n",
                "n_features = data_info['n_features']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# PYTORCH BENCHMARK\n",
                "# ==============================================================================\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "print(\"\\n[PyTorch Benchmark]\")\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "\n",
                "pytorch_results = {}\n",
                "\n",
                "# Convert to tensors\n",
                "X_train_t = torch.FloatTensor(X_train).to(device)\n",
                "y_train_t = torch.FloatTensor(y_train).to(device).view(-1, 1)\n",
                "X_test_t = torch.FloatTensor(X_test).to(device)\n",
                "\n",
                "# Method 1: torch.linalg.lstsq\n",
                "print(\"Testing torch.linalg.lstsq...\")\n",
                "start = time.time()\n",
                "X_train_bias = torch.cat([torch.ones(X_train_t.shape[0], 1).to(device), X_train_t], dim=1)\n",
                "solution = torch.linalg.lstsq(X_train_bias, y_train_t).solution\n",
                "fit_time = time.time() - start\n",
                "\n",
                "start = time.time()\n",
                "X_test_bias = torch.cat([torch.ones(X_test_t.shape[0], 1).to(device), X_test_t], dim=1)\n",
                "y_pred = X_test_bias @ solution\n",
                "predict_time = time.time() - start\n",
                "\n",
                "y_pred_np = y_pred.cpu().numpy().flatten()\n",
                "mse = np.mean((y_test - y_pred_np) ** 2)\n",
                "r2 = 1 - np.sum((y_test - y_pred_np) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
                "\n",
                "pytorch_results['PyTorch_LSTSQ'] = {\n",
                "    'fit_time': fit_time,\n",
                "    'predict_time': predict_time,\n",
                "    'mse': float(mse),\n",
                "    'rmse': float(np.sqrt(mse)),\n",
                "    'mae': float(np.mean(np.abs(y_test - y_pred_np))),\n",
                "    'r2': float(r2),\n",
                "    'device': str(device)\n",
                "}\n",
                "print(f\"  MSE: {mse:.6f}, R²: {r2:.6f}\")\n",
                "\n",
                "# Method 2: nn.Linear with SGD\n",
                "print(\"Testing nn.Linear with SGD...\")\n",
                "\n",
                "class LinearModel(nn.Module):\n",
                "    def __init__(self, n_features):\n",
                "        super().__init__()\n",
                "        self.linear = nn.Linear(n_features, 1)\n",
                "    def forward(self, x):\n",
                "        return self.linear(x)\n",
                "\n",
                "model = LinearModel(n_features).to(device)\n",
                "criterion = nn.MSELoss()\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
                "\n",
                "start = time.time()\n",
                "for epoch in range(1000):\n",
                "    model.train()\n",
                "    optimizer.zero_grad()\n",
                "    outputs = model(X_train_t)\n",
                "    loss = criterion(outputs, y_train_t)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "fit_time = time.time() - start\n",
                "\n",
                "start = time.time()\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    y_pred = model(X_test_t)\n",
                "predict_time = time.time() - start\n",
                "\n",
                "y_pred_np = y_pred.cpu().numpy().flatten()\n",
                "mse = np.mean((y_test - y_pred_np) ** 2)\n",
                "r2 = 1 - np.sum((y_test - y_pred_np) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
                "\n",
                "pytorch_results['PyTorch_SGD'] = {\n",
                "    'fit_time': fit_time,\n",
                "    'predict_time': predict_time,\n",
                "    'mse': float(mse),\n",
                "    'rmse': float(np.sqrt(mse)),\n",
                "    'mae': float(np.mean(np.abs(y_test - y_pred_np))),\n",
                "    'r2': float(r2),\n",
                "    'epochs': 1000,\n",
                "    'device': str(device)\n",
                "}\n",
                "print(f\"  MSE: {mse:.6f}, R²: {r2:.6f}\")\n",
                "print(\"[OK] PyTorch benchmark complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# TENSORFLOW BENCHMARK\n",
                "# ==============================================================================\n",
                "\n",
                "import tensorflow as tf\n",
                "\n",
                "print(\"\\n[TensorFlow Benchmark]\")\n",
                "physical_devices = tf.config.list_physical_devices('GPU')\n",
                "device_name = 'GPU' if len(physical_devices) > 0 else 'CPU'\n",
                "print(f\"Device: {device_name}\")\n",
                "\n",
                "tf_results = {}\n",
                "\n",
                "# Convert to tensors\n",
                "X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
                "y_train_tf = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
                "X_test_tf = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
                "\n",
                "# Method 1: tf.linalg.lstsq\n",
                "print(\"Testing tf.linalg.lstsq...\")\n",
                "start = time.time()\n",
                "X_train_bias = tf.concat([tf.ones((X_train_tf.shape[0], 1)), X_train_tf], axis=1)\n",
                "solution = tf.linalg.lstsq(X_train_bias, tf.expand_dims(y_train_tf, axis=-1))\n",
                "fit_time = time.time() - start\n",
                "\n",
                "start = time.time()\n",
                "X_test_bias = tf.concat([tf.ones((X_test_tf.shape[0], 1)), X_test_tf], axis=1)\n",
                "y_pred = tf.matmul(X_test_bias, solution)\n",
                "predict_time = time.time() - start\n",
                "\n",
                "y_pred_np = y_pred.numpy().flatten()\n",
                "mse = np.mean((y_test - y_pred_np) ** 2)\n",
                "r2 = 1 - np.sum((y_test - y_pred_np) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
                "\n",
                "tf_results['TensorFlow_LSTSQ'] = {\n",
                "    'fit_time': fit_time,\n",
                "    'predict_time': predict_time,\n",
                "    'mse': float(mse),\n",
                "    'rmse': float(np.sqrt(mse)),\n",
                "    'mae': float(np.mean(np.abs(y_test - y_pred_np))),\n",
                "    'r2': float(r2),\n",
                "    'device': device_name\n",
                "}\n",
                "print(f\"  MSE: {mse:.6f}, R²: {r2:.6f}\")\n",
                "\n",
                "# Method 2: Keras Sequential\n",
                "print(\"Testing Keras Sequential...\")\n",
                "start = time.time()\n",
                "inputs = tf.keras.Input(shape=(n_features,))\n",
                "outputs = tf.keras.layers.Dense(1, use_bias=True)(inputs)\n",
                "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
                "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01), loss='mse')\n",
                "history = model.fit(X_train_tf, y_train_tf, epochs=100, batch_size=32, verbose=0)\n",
                "fit_time = time.time() - start\n",
                "\n",
                "start = time.time()\n",
                "y_pred = model.predict(X_test_tf, verbose=0)\n",
                "predict_time = time.time() - start\n",
                "\n",
                "y_pred_np = y_pred.flatten()\n",
                "mse = np.mean((y_test - y_pred_np) ** 2)\n",
                "r2 = 1 - np.sum((y_test - y_pred_np) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
                "\n",
                "tf_results['TensorFlow_Keras'] = {\n",
                "    'fit_time': fit_time,\n",
                "    'predict_time': predict_time,\n",
                "    'mse': float(mse),\n",
                "    'rmse': float(np.sqrt(mse)),\n",
                "    'mae': float(np.mean(np.abs(y_test - y_pred_np))),\n",
                "    'r2': float(r2),\n",
                "    'epochs': 100,\n",
                "    'device': device_name\n",
                "}\n",
                "print(f\"  MSE: {mse:.6f}, R²: {r2:.6f}\")\n",
                "print(\"[OK] TensorFlow benchmark complete\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# LOUSYBOOKML BENCHMARK (Optional - for same-environment comparison)\n",
                "# ==============================================================================\n",
                "\n",
                "try:\n",
                "    from lousybookml.linear_model import LinearRegression, GradientDescentRegressor, Ridge\n",
                "    from lousybookml import metrics\n",
                "    \n",
                "    print(\"\\n[LousyBookML Benchmark]\")\n",
                "    lb_results = {}\n",
                "    \n",
                "    # OLS\n",
                "    print(\"Testing LinearRegression...\")\n",
                "    start = time.time()\n",
                "    lr = LinearRegression()\n",
                "    lr.fit(X_train, y_train)\n",
                "    fit_time = time.time() - start\n",
                "    \n",
                "    start = time.time()\n",
                "    y_pred = lr.predict(X_test)\n",
                "    predict_time = time.time() - start\n",
                "    \n",
                "    lb_results['LousyBookML_OLS'] = {\n",
                "        'fit_time': fit_time,\n",
                "        'predict_time': predict_time,\n",
                "        'mse': float(metrics.mse(y_test, y_pred)),\n",
                "        'rmse': float(metrics.rmse(y_test, y_pred)),\n",
                "        'mae': float(metrics.mae(y_test, y_pred)),\n",
                "        'r2': float(metrics.r2_score(y_test, y_pred))\n",
                "    }\n",
                "    print(f\"  MSE: {lb_results['LousyBookML_OLS']['mse']:.6f}, R²: {lb_results['LousyBookML_OLS']['r2']:.6f}\")\n",
                "    \n",
                "    # Gradient Descent\n",
                "    print(\"Testing GradientDescentRegressor...\")\n",
                "    start = time.time()\n",
                "    gd = GradientDescentRegressor(learning_rate=0.01, n_iterations=1000)\n",
                "    gd.fit(X_train, y_train)\n",
                "    fit_time = time.time() - start\n",
                "    \n",
                "    start = time.time()\n",
                "    y_pred = gd.predict(X_test)\n",
                "    predict_time = time.time() - start\n",
                "    \n",
                "    lb_results['LousyBookML_GD'] = {\n",
                "        'fit_time': fit_time,\n",
                "        'predict_time': predict_time,\n",
                "        'mse': float(metrics.mse(y_test, y_pred)),\n",
                "        'rmse': float(metrics.rmse(y_test, y_pred)),\n",
                "        'mae': float(metrics.mae(y_test, y_pred)),\n",
                "        'r2': float(metrics.r2_score(y_test, y_pred)),\n",
                "        'iterations': gd.n_iter_\n",
                "    }\n",
                "    print(f\"  MSE: {lb_results['LousyBookML_GD']['mse']:.6f}, R²: {lb_results['LousyBookML_GD']['r2']:.6f}\")\n",
                "    \n",
                "    # Ridge\n",
                "    print(\"Testing Ridge...\")\n",
                "    start = time.time()\n",
                "    ridge = Ridge(alpha=1.0)\n",
                "    ridge.fit(X_train, y_train)\n",
                "    fit_time = time.time() - start\n",
                "    \n",
                "    start = time.time()\n",
                "    y_pred = ridge.predict(X_test)\n",
                "    predict_time = time.time() - start\n",
                "    \n",
                "    lb_results['LousyBookML_Ridge'] = {\n",
                "        'fit_time': fit_time,\n",
                "        'predict_time': predict_time,\n",
                "        'mse': float(metrics.mse(y_test, y_pred)),\n",
                "        'rmse': float(metrics.rmse(y_test, y_pred)),\n",
                "        'mae': float(metrics.mae(y_test, y_pred)),\n",
                "        'r2': float(metrics.r2_score(y_test, y_pred))\n",
                "    }\n",
                "    print(f\"  MSE: {lb_results['LousyBookML_Ridge']['mse']:.6f}, R²: {lb_results['LousyBookML_Ridge']['r2']:.6f}\")\n",
                "    print(\"[OK] LousyBookML benchmark complete\")\n",
                "    \n",
                "except ImportError as e:\n",
                "    print(f\"[!] LousyBookML not available: {e}\")\n",
                "    lb_results = {}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ==============================================================================\n",
                "# SAVE RESULTS\n",
                "# ==============================================================================\n",
                "\n",
                "# Combine all results\n",
                "all_results = {}\n",
                "all_results.update(pytorch_results)\n",
                "all_results.update(tf_results)\n",
                "if 'lb_results' in locals():\n",
                "    all_results.update(lb_results)\n",
                "\n",
                "# Create output\n",
                "output = {\n",
                "    'data_info': {\n",
                "        'n_samples': data_info['n_samples'],\n",
                "        'n_features': data_info['n_features'],\n",
                "        'noise': data_info['noise'],\n",
                "        'random_state': data_info['random_state'],\n",
                "        'train_size': data_info['train_size'],\n",
                "        'test_size': data_info['test_size']\n",
                "    },\n",
                "    'results': all_results,\n",
                "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
                "    'environment': 'Google Colab',\n",
                "    'pytorch_version': str(torch.__version__),\n",
                "    'tensorflow_version': str(tf.__version__)\n",
                "}\n",
                "\n",
                "# Save to SINGLE file (contains both data and results)\n",
                "with open('colab_benchmark.json', 'w') as f:\n",
                "    json.dump(output, f, indent=2)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"BENCHMARK COMPLETE\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\nTotal models benchmarked: {len(all_results)}\")\n",
                "print(\"\\nFile to download:\")\n",
                "print(\"  colab_benchmark.json - contains BOTH data and results\")\n",
                "print(\"\\nPlace in your local benchmark/ folder\")\n",
                "print(\"Then run: python benchmark/benchmark_runner.py\")\n",
                "\n",
                "# Display summary table\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "print(f\"{'Model':<30} {'Fit Time (s)':<15} {'MSE':<12} {'R²':<10}\")\n",
                "print(\"-\"*80)\n",
                "for model_name, result in all_results.items():\n",
                "    fit_time = result.get('fit_time', 0)\n",
                "    mse = result.get('mse', 0)\n",
                "    r2 = result.get('r2', 0)\n",
                "    print(f\"{model_name:<30} {fit_time:<15.6f} {mse:<12.6f} {r2:<10.6f}\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Download Results\n",
                "\n",
                "Run the cell below to download the single file, or click the folder icon on the left and download manually."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download file automatically\n",
                "from google.colab import files\n",
                "\n",
                "print(\"Downloading colab_benchmark.json...\")\n",
                "files.download('colab_benchmark.json')\n",
                "\n",
                "print(\"\\n[OK] File downloaded!\")"
            ]
        }
    ]
}